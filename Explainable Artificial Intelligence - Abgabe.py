# -*- coding: utf-8 -*-
"""Karaoglan-Masterarbeit-XAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IZ6ABbo0oE9VBIatAv6I4GHUZoBbhKA4

# **Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# # LIME and SHAP packages have to be installed via pip
# # The %%capture command hide code cell output in Google Colab
# %%capture 
# !pip install lime
# !pip install shap

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import io

import pandas as pd
import numpy as np 
import tensorflow as tf
import time

# Plot results
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Data preparation and pre-processing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Model classifiers
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

# Classifier metrics 
from sklearn import metrics
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score

# Resample dataset
import collections
from imblearn.over_sampling import SMOTE

# Explainability
import lime
from lime.lime_tabular import LimeTabularExplainer
from lime import submodular_pick
import shap

import warnings
warnings.filterwarnings("ignore")

"""# **Data Upload and Cleaning**"""

# Upload the Credit Card Default dataset with google.colab.files
# Wait till the upload is 100%
uploaded = files.upload()

# Read the dataset from the excel file 
data = pd.read_csv(io.BytesIO(uploaded['DefaultOfCreditCardClients.csv']), sep=";", header=1)

data.head(3)

# Dataset without the ID column
data.drop('ID',axis=1, inplace=True)

# Checking missing values - there aren't any non-null values
#data.isnull().sum()

# Statistical description
data.describe()
# There are unusal values for PAY_0-PAY_6 the -2, for MARRIAGE the 0 or EDUCATION the 6
# Therefore clean the data

# Rename PAY_0 and Target column
data = data.rename(columns={'PAY_0': 'PAY_1', 'default payment next month': 'Default Payment'})

data.loc[data['SEX']==2,'SEX'] = 0
data.loc[data['MARRIAGE'] == 0, 'MARRIAGE'] = 3
clean_education = (data['EDUCATION'] == 0) | (data['EDUCATION'] == 5) | (data['EDUCATION'] == 6)
data.loc[clean_education, 'EDUCATION'] = 4

clean_pay1 = (data['PAY_1'] == -2) | (data['PAY_1'] == -1)
data.loc[clean_pay1, 'PAY_1'] = 0
clean_pay2 = (data['PAY_2'] == -2) | (data['PAY_2'] == -1)
data.loc[clean_pay2, 'PAY_2'] = 0
clean_pay3 = (data['PAY_3'] == -2) | (data['PAY_3'] == -1)
data.loc[clean_pay3, 'PAY_3'] = 0
clean_pay4 = (data['PAY_4'] == -2) | (data['PAY_4'] == -1)
data.loc[clean_pay4, 'PAY_4'] = 0
clean_pay5 = (data['PAY_5'] == -2) | (data['PAY_5'] == -1)
data.loc[clean_pay5, 'PAY_5'] = 0
clean_pay6 = (data['PAY_6'] == -2) | (data['PAY_6'] == -1)
data.loc[clean_pay6, 'PAY_6'] = 0
#data[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].describe()

"""# **Simple Exploratory Data Analysis**"""

print(data['SEX'].value_counts())
plt.figure(figsize=(8,4))
sns.countplot(x='SEX', data=data, hue='Default Payment', palette='bright')

print(data['MARRIAGE'].value_counts())
plt.figure(figsize=(8,4))
sns.countplot(x='MARRIAGE', data=data, hue='Default Payment', palette='bright')

print(data['EDUCATION'].value_counts())
plt.figure(figsize=(8,4))
sns.countplot(x='EDUCATION', data=data, hue='Default Payment', palette='bright')

fig, ax =plt.subplots(2,3)
fig.set_size_inches(16,8)
sns.countplot(x='PAY_1', data=data, hue='Default Payment', palette='bright', ax=ax[0,0]).legend(loc="upper right")
sns.countplot(x='PAY_2', data=data, hue='Default Payment', palette='bright', ax=ax[0,1]).legend(loc="upper right")
sns.countplot(x='PAY_3', data=data, hue='Default Payment', palette='bright', ax=ax[0,2]).legend(loc="upper right")
sns.countplot(x='PAY_4', data=data, hue='Default Payment', palette='bright', ax=ax[1,0]).legend(loc="upper right")
sns.countplot(x='PAY_5', data=data, hue='Default Payment', palette='bright', ax=ax[1,1]).legend(loc="upper right")
sns.countplot(x='PAY_6', data=data, hue='Default Payment', palette='bright', ax=ax[1,2]).legend(loc="upper right")
fig.tight_layout()
fig.show()

#data[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].describe()

#data[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].describe()

#data[['LIMIT_BAL']].describe()

#plt.figure(figsize=(16,8))
#sns.countplot(x='AGE', data=data, hue='Default Payment', palette='bright')

# Correlation analysis 
plt.subplots(figsize=(30,10))
sns.heatmap(data.corr(), square=True, annot=True, fmt=".1f", cmap="YlGnBu")
# It seems that PAY_1 has the highest correlation to the target Default Payment

"""# **Simple Feature Engineering and Preprocessing**"""

Y = data['Default Payment']
#Y.head()

# Order features first categorical and second continuous
feature_order = ['SEX',
                 'EDUCATION',
                 'MARRIAGE',
                 'PAY_1',
                 'PAY_2',
                 'PAY_3',
                 'PAY_4',
                 'PAY_5',
                 'PAY_6',
                 'LIMIT_BAL', 
                 'AGE',
                 'BILL_AMT1',
                 'BILL_AMT2',
                 'BILL_AMT3',
                 'BILL_AMT4',
                 'BILL_AMT5',
                 'BILL_AMT6',
                 'PAY_AMT1',
                 'PAY_AMT2',
                 'PAY_AMT3',
                 'PAY_AMT4',
                 'PAY_AMT5',
                 'PAY_AMT6']

X = data[feature_order]
#X.head()

# List of categorical features - preparation for LIME input
categorical_names = {}
categorical_names[0] = ['female', 'male']
categorical_names[1] = [1, 2, 3, 4]
categorical_names[2] = [1, 2, 3]
categorical_names[3] = [0, 1, 2, 3, 4, 5, 6, 7, 8]
categorical_names[4] = [0, 1, 2, 3, 4, 5, 6, 7, 8]
categorical_names[5] = [0, 1, 2, 3, 4, 5, 6, 7, 8]
categorical_names[6] = [0, 1, 2, 3, 4, 5, 6, 7, 8]
categorical_names[7] = [0, 1, 2, 3, 4, 5, 6, 7, 8]
categorical_names[8] = [0, 1, 2, 3, 4, 5, 6, 7, 8]

# List of continuous features
continuous_features = list(X.columns[9:])

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))

def scaleColumns(X, cols_to_scale):
  for col in cols_to_scale:
    X[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(X[col])),
                          columns=[col])
    return X

X_scaled = scaleColumns(X,[continuous_features])
#X_scaled.head()

# Split the data into train/test datasets
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=12345)
#print(len(X_train)) 24000
#print(len(X_test)) 6000

"""# **Resampling**"""

# Set random state and make the outputs stable
np.random.seed(12345)

# Number of default payment and the ratio of it
# clearly imbalanced data, therefore resample with SMOTE
print(data["Default Payment"].value_counts())
print("Default Payment Percentage 0: {0:.2f} %".format(data[data["Default Payment"]==0].shape[0] / data.shape[0] * 100) )
print("Default Payment Percentage 1: {0:.2f} %".format(data[data["Default Payment"]==1].shape[0] / data.shape[0] * 100) )

print(collections.Counter(Y_train))
#print(len(Y_train)) 24000
# The minority class 1 have just 5331 instances while the majority class 0 have 18669

# Resample the train data
X_resampled, Y_resampled = SMOTE().fit_sample(X_train, Y_train)
# Convert the data to the same type as before SMOTE
X_train = pd.DataFrame(X_resampled, columns=feature_order) 
Y_train = pd.Series(Y_resampled)

print(collections.Counter(Y_train))
#print(len(Y_train)) 37338
# After applying SMOTE Method the classes are balanced

# Already tested the Black-Box models for unbalanced and balanced data 
# --> the balanced data have better recall and f1 score 
# Metrics without SMOTE:
# Model  Precision   Recall  F1 Score  Accuracy       ROC
#  SVC   0.651106  0.406130  0.500236  0.823500  0.672820
#  RFC   0.637191  0.375479  0.472517  0.817667  0.658027
#  MLP   0.660274  0.369349  0.473710  0.821500  0.658263

# Metrics with SMOTE:
#Model   Precision   Recall  F1 Score  Accuracy       ROC
#  SVC   0.545605  0.504215  0.524094  0.800833  0.693747
#  RFC   0.521073  0.521073  0.521073  0.791667  0.693976
#  MLP   0.403059  0.686590  0.507937  0.710667  0.701974

"""# **Black-Box Model Training**"""

# Commented out IPython magic to ensure Python compatibility.
# Fitting a Support Vector Machine Classifier
# Actually the SVC model don't need so much computing time 
# but the lime package requires probabilities and 
# therefore the default probability=False has to be changed to True
svc = SVC(kernel='linear', probability=True, random_state=12345)
# %time svc.fit(X_train, Y_train)
svc_pred = svc.predict(X_test)

# Confusion Matrix
pd.crosstab(Y_test, svc_pred, rownames=['Actual'], colnames=['Predicted'])

# Model performs
print(classification_report(Y_test, svc_pred))

# Commented out IPython magic to ensure Python compatibility.
# Fitting a Random Forest Classifier
rfc = RandomForestClassifier(n_estimators=150, criterion='entropy', random_state=12345)
# %time rfc.fit(X_train, Y_train)
rfc_pred = rfc.predict(X_test)

# Confusion Matrix
pd.crosstab(Y_test, rfc_pred, rownames=['Actual'], colnames=['Predicted'])

# Model performs
print(classification_report(Y_test, rfc_pred))

# Commented out IPython magic to ensure Python compatibility.
# Fitting a Multi-Layer Perceptron Classifier
mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000, random_state=12345)
# %time mlp.fit(X_train, Y_train.values.ravel())
mlp_pred = mlp.predict(X_test)

# Confusion Matrix
pd.crosstab(Y_test, mlp_pred, rownames=['Actual'], colnames=['Predicted'])

# Model performs
print(classification_report(Y_test,mlp_pred))

# Get a table for the metrics of the model performs
svc_prec = precision_score(Y_test, svc_pred)
svc_rec = recall_score(Y_test, svc_pred)
svc_f1 = f1_score(Y_test, svc_pred)
svc_acc = accuracy_score(Y_test, svc_pred)
svc_roc = roc_auc_score(Y_test, svc_pred)

rfc_prec = precision_score(Y_test, rfc_pred)
rfc_rec = recall_score(Y_test, rfc_pred)
rfc_f1 = f1_score(Y_test, rfc_pred)
rfc_acc = accuracy_score(Y_test, rfc_pred)
rfc_roc = roc_auc_score(Y_test, rfc_pred)

mlp_prec = precision_score(Y_test, mlp_pred)
mlp_rec = recall_score(Y_test, mlp_pred)
mlp_f1 = f1_score(Y_test, mlp_pred)
mlp_acc = accuracy_score(Y_test, mlp_pred)
mlp_roc = roc_auc_score(Y_test, mlp_pred)

metric = pd.DataFrame([['SVC', svc_prec, svc_rec, svc_f1, svc_acc, svc_roc]],
                      columns = ['Model', 'Precision', 'Recall', 'F1 Score', 'Accuracy', 'ROC'])

metric2 = pd.DataFrame([['RFC', rfc_prec, rfc_rec, rfc_f1, rfc_acc, rfc_roc]],
                      columns = ['Model', 'Precision', 'Recall', 'F1 Score', 'Accuracy', 'ROC'])

metric3 = pd.DataFrame([['MLP', mlp_prec, mlp_rec, mlp_f1, mlp_acc, mlp_roc]],
                      columns = ['Model', 'Precision', 'Recall', 'F1 Score', 'Accuracy', 'ROC'])

metric = metric.append([metric2, metric3], sort=False)
metric

# ROC Curve
probs_svc = svc.predict_proba(X_test)[:,1]
FPR1, TPR1, _ = metrics.roc_curve(Y_test, probs_svc)

probs_rfc = rfc.predict_proba(X_test)[:,1]
FPR2, TPR2, _ = metrics.roc_curve(Y_test, probs_rfc)

probs_mlp = mlp.predict_proba(X_test)[:,1]
FPR3, TPR3, _ = metrics.roc_curve(Y_test, probs_mlp)

plt.figure(figsize=(10,8))
plt.plot([0, 1], [0, 1], 'r--')
plt.plot(FPR1,TPR1,label="SVC, auc="+str(round(svc_roc,3)))
plt.plot(FPR2,TPR2,label="RFC, auc="+str(round(rfc_roc,3)))
plt.plot(FPR3,TPR3,label="MLP, auc="+str(round(mlp_roc,3)))
plt.legend(loc=4, title='Models', facecolor='white')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC', size=15)

"""# **Explainability**

## *Compare the LIME explanations of the Black-Box models at 4 different instances, additionally the Kernel SHAP explanation for MLP*
"""

predict_svc = lambda x: svc.predict_proba(x).astype(float)
predict_rfc = lambda x: rfc.predict_proba(x).astype(float)
predict_mlp = lambda x: mlp.predict_proba(x).astype(float)
#rfc.predict(X_test) array([1, 0, 0, ..., 0, 0, 0])
#rfc.predict_proba(X_test) array([[0.31333333, 0.68666667], [0.86, 0.14], [0.8, 0.2], ..]

explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        verbose=True)
# verbose = True, so the intercept the LIME and Black-Box Modell prediction will be shown

"""Instance 1  - Actual Label: 0, Predicted Label: 0"""

#i = 35
#print(X_test.iloc[i]) shows the instance as the actual data point 
#print(Y_test.iloc[i])     0
#print(svc_pred[i])        0
#print(rfc_pred[i])        0
#print(mlp_pred[i])        0

# Commented out IPython magic to ensure Python compatibility.
i = 35
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_svc, num_features=5)
print(exp.score)
exp.show_in_notebook()
#exp.local_exp
exp.as_list()

# The prediction of LIME model is the sum of the intercept and coefficients
0.8694245959934388 + (-0.31115890778276223) + (-0.20141194001493934) + 0.003953150195173049 + 0.00391378520958269 + 0.00378764933835298

# Commented out IPython magic to ensure Python compatibility.
i = 35
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_rfc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 35
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer = shap.KernelExplainer(predict_mlp, X_train)
shap.initjs()

i = 35
# %time shap_values = explainer.shap_values(X_test.iloc[i], nsamples=100)
shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[i], link="identity")

shap_values[1]

"""Instance 2 - Actual Label: 1, Predicted Label: 1"""

#i = 2675
#print(X_test.iloc[i])
#print(Y_test.iloc[i])     1
#print(svc_pred[i])        1
#print(rfc_pred[i])        1
#print(mlp_pred[i])        1

# Commented out IPython magic to ensure Python compatibility.
i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_svc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_rfc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer = shap.KernelExplainer(predict_mlp, X_train)
shap.initjs()

i = 2675
# %time shap_values = explainer.shap_values(X_test.iloc[i], nsamples=100)
shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[i], link="identity")

shap_values[1]

"""Instance 3 - Actual Label: 0, Predicted Label: 1"""

#i = 555
#print(X_test.iloc[i])
#print(Y_test.iloc[i])    0
#print(svc_pred[i])       1
#print(rfc_pred[i])       1
#print(mlp_pred[i])       1

# Commented out IPython magic to ensure Python compatibility.
i = 555
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_svc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 555
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_rfc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 555
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer = shap.KernelExplainer(predict_mlp, X_train)
shap.initjs()

i = 555
# %time shap_values = explainer.shap_values(X_test.iloc[i], nsamples=100)
shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[i], link="identity")

shap_values[1]

"""Instance 4 - Actual Label: 1, Predicted Label: 0"""

#i = 1880
#print(X_test.iloc[i])
#print(Y_test.iloc[i])    1
#print(svc_pred[i])       0
#print(rfc_pred[i])       0
#print(mlp_pred[i])       0

# Commented out IPython magic to ensure Python compatibility.
i = 1880
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_svc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 1880
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_rfc, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
i = 1880
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer = shap.KernelExplainer(predict_mlp, X_train)
shap.initjs()

i = 1880
# %time shap_values = explainer.shap_values(X_test.iloc[i], nsamples=100)
shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[i], link="identity")

shap_values[1]

"""## *Choose the MLP model at the Instance 2 and compare LIME results for different kernels*"""

# Commented out IPython magic to ensure Python compatibility.
explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        kernel_width=5, 
                                                        verbose=True)

i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        kernel_width=4, 
                                                        verbose=True)

i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        kernel_width=3, 
                                                        verbose=True)

i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        kernel_width=2, 
                                                        verbose=True)

i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

# Commented out IPython magic to ensure Python compatibility.
explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        kernel_width=1, 
                                                        verbose=True)

i = 2675
# %time exp = explainer_lime.explain_instance(X_test.iloc[i], predict_fn = predict_mlp, num_features=5)
print(exp.score)
exp.show_in_notebook()

"""## *Compare the SP-LIME across the three Black-Box models, additionally the global Kernel SHAP explanation for MLP*"""

explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, 
                                                        class_names=Y_train.unique(), 
                                                        feature_names = X_train.columns,
                                                        categorical_features = categorical_names, 
                                                        verbose=False)

#verbose=false, so the information about local explainder don't show up

# Commented out IPython magic to ensure Python compatibility.
# %time sp_obj = submodular_pick.SubmodularPick(explainer_lime, X_train.values, predict_svc, sample_size=20, num_features=5, num_exps_desired=2)
[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations]

# Commented out IPython magic to ensure Python compatibility.
# %time sp_obj = submodular_pick.SubmodularPick(explainer_lime, X_train.values, predict_rfc, sample_size=20, num_features=5, num_exps_desired=2)
[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations]

# Commented out IPython magic to ensure Python compatibility.
# %time sp_obj = submodular_pick.SubmodularPick(explainer_lime, X_train.values, predict_mlp, sample_size=20, num_features=5, num_exps_desired=2)
[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations]

# Sampling data from the training set to reduce time
# Running without the kmeans end up with ram crash
X_train_summary = shap.kmeans(X_train, 10)

# Commented out IPython magic to ensure Python compatibility.
explainer = shap.KernelExplainer(predict_mlp, X_train_summary)
# %time shap_values = explainer.shap_values(X_test)
shap.initjs()
shap.summary_plot(shap_values, X_test)